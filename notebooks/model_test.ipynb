{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72c985f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zamko\\Documents\\CaylePy\\cayleypy\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cayleypy import PermutationGroups, CayleyGraph\n",
    "import pandas as pd\n",
    "from cayleypy.models.models import ModelConfig, MlpModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "195a31f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_k(n):\n",
    "    # мне нужно вернуть лист из потенциальных валидных k. k является валидным если наибольший общий делитель n и k равен 1\n",
    "    valid_k = []\n",
    "    for k in range(1, n):\n",
    "        if math.gcd(n, k) == 1:\n",
    "            valid_k.append(k)\n",
    "    return valid_k\n",
    "\n",
    "\n",
    "def get_n_long_permutations(n):\n",
    "    \"\"\"Generates long permutations for n elements.\"\"\"\n",
    "    list_long_permutations = []\n",
    "    k = int(n/2)\n",
    "    if n == 2*k + 1:  # For odd \"n\"\n",
    "        for element_index in range(n):\n",
    "            p = np.arange(n)\n",
    "            for i in range(k):\n",
    "                p[element_index-i], p[(element_index+1+i)%n] = p[(element_index+1+i)%n], p[element_index-i]\n",
    "            list_long_permutations.append(p)\n",
    "    else:\n",
    "        # First generate permutations corresponding to symmetries NOT passing by edges of n-gon\n",
    "        for element_index in range(int(n/2)): \n",
    "            p = np.arange(n)\n",
    "            for i in range(k):\n",
    "                p[element_index-i], p[(element_index+1+i)%n] = p[(element_index+1+i)%n], p[element_index-i]\n",
    "            list_long_permutations.append(p)\n",
    "        # Second generate those which correspond to diagonal passing through the nodes\n",
    "        for element_index in range(int(n/2)): \n",
    "            p = np.arange(n)\n",
    "            for i in range(1, k+1):\n",
    "                p[element_index-i], p[(element_index+i)%n] = p[(element_index+i)%n], p[element_index-i]\n",
    "            list_long_permutations.append(p)    \n",
    "            \n",
    "    return list_long_permutations\n",
    "\n",
    "def train_epoch(model, X, y, batch_size, criterion, optimizer, epoch):\n",
    "    y_train = y.float()\n",
    "    indices = torch.randperm(X.shape[0], dtype=torch.int64, device='cpu')  # Generate indices on CPU\n",
    "    X_train = X[indices]\n",
    "    y_train = y_train[indices]\n",
    "    \n",
    "    model.train()\n",
    "    # Neural network train by batches (not to crash RAM)\n",
    "    n_states_all =  X_train.shape[0]\n",
    "    cc = 0; train_loss =    0.0\n",
    "    for i_start_batch  in range(0,n_states_all,batch_size ):\n",
    "        i_end_batch = min( [i_start_batch + batch_size,  n_states_all ] )\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(X_train[i_start_batch:i_end_batch])\n",
    "        loss = criterion(outputs.squeeze(), y_train[i_start_batch:i_end_batch])\n",
    "\n",
    "        # Backward and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item(); \n",
    "        cc+=1\n",
    "    train_loss /= cc\n",
    "    \n",
    "    # print(f\"Epoch {epoch:2d} | \"\n",
    "    #       f\"Train: {train_loss:.4f} | \")\n",
    "    return train_loss\n",
    "\n",
    "def initialize_states(list_generators, device):\n",
    "    n = len(list_generators[0])\n",
    "    p = np.arange(n)\n",
    "    # Swap (0,2)\n",
    "    p[0], p[1] = p[1], p[0]\n",
    "    i = 2\n",
    "    while i < n-i+1:\n",
    "        #print(i, n-i+1)\n",
    "        p[i], p[n-i+1] = p[n-i+1], p[i]\n",
    "        i += 1\n",
    "    permutation_longest = torch.tensor(p, dtype=torch.int64, device=device)\n",
    "    state_start = permutation_longest\n",
    "\n",
    "    state_destination = torch.arange(len(list_generators[0]), device=device, dtype=torch.int64)\n",
    "    \n",
    "    return state_start, state_destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1907bdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================= ОПРЕДЕЛЕНИЕ ПАРАМЕТРОВ =======================\n",
    "\n",
    "# Параметры графа\n",
    "n = 15\n",
    "valid_k = get_valid_k(n)\n",
    "k = valid_k[0]\n",
    "\n",
    "CFG = dict()\n",
    "\n",
    "# Параметры сети\n",
    "CFG[\"model_type\"] = \"MLP\"\n",
    "CFG[\"input_size\"] = n\n",
    "CFG[\"num_classes_for_one_hot\"] = n\n",
    "CFG[\"layers_sizes\"] = [4096]\n",
    "CFG[\"weights_kaggle_id\"] = None\n",
    "CFG[\"weights_path\"] = None\n",
    "CFG[\"learning_rate\"] = 0.001\n",
    "# Параметры сети\n",
    "\n",
    "# Параметры обучения\n",
    "CFG[\"batch_size\"] = 1024\n",
    "CFG[\"num_epochs\"] = 100\n",
    "\n",
    "# Параметры beam search\n",
    "CFG[\"num_start_states\"] = 5\n",
    "CFG[\"beam_size_range\"] = [10**2]\n",
    "CFG[\"max_iterations\"] = 1024\n",
    "CFG[\"return_path\"] = True\n",
    "CFG[\"bfs_result_for_mitm\"] = None\n",
    "\n",
    "# random_walks parameters\n",
    "CFG[\"k\"] = k\n",
    "CFG[\"random_walks_width\"] = 1024\n",
    "CFG[\"random_walks_length\"] = n * (n - 1) // 2\n",
    "\n",
    "CFG_model = ModelConfig.from_dict(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a3ff118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MLP with random walks: 100%|██████████| 100/100 [00:40<00:00,  2.46it/s, loss=207.4911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting beam search with hamming distance predictor...\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# ======================= ОСНОВНАЯ ЛОГИКА =======================\n",
    "df_result = pd.DataFrame(columns=[\"n\", \"k\", \"start_state\", \"beam_size\", \"path_found\", \"path_length\", \"path\"])\n",
    "\n",
    "# 1. Создание графа\n",
    "graph = CayleyGraph(PermutationGroups.lrx(CFG[\"input_size\"], CFG[\"k\"]), bit_encoding_width = None)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. Создание модели\n",
    "model = MlpModel(CFG_model).to(device)\n",
    "\n",
    "# 4. Определение функции потерь и оптимизатора\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG[\"learning_rate\"], weight_decay= 1e-5)\n",
    "\n",
    "# 5. Цикл обучения\n",
    "pbar = tqdm(range(CFG[\"num_epochs\"]), desc=\"Training MLP with random walks\")\n",
    "for epoch in pbar:\n",
    "    X_tr, y_tr = graph.random_walks(width=CFG[\"random_walks_width\"], length=CFG[\"random_walks_length\"], mode=\"bfs\")\n",
    "    y_tr = y_tr.float()\n",
    "    loss = train_epoch(model, X_tr, y_tr, CFG[\"batch_size\"], loss_fn, optimizer, epoch)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        # обновляем только postfix, не ломая самую прогресс-строку\n",
    "        pbar.set_postfix(loss=f\"{loss:.4f}\")\n",
    "\n",
    "# 6. Запуск beam search и сравнение методов\n",
    "print(\"Starting beam search with hamming distance predictor...\")\n",
    "nn_success_rates = {}\n",
    "list_long_permutations = get_n_long_permutations(n)\n",
    "list_long_permutations = [torch.tensor(start_state, dtype=torch.int64) for start_state in list_long_permutations]\n",
    "for start_state in list_long_permutations:\n",
    "    nn_success_rates[start_state] = graph.beam_search(start_state=start_state, predictor=model, beam_width=CFG[\"beam_size_range\"][-1], max_iterations=CFG[\"max_iterations\"], return_path=CFG[\"return_path\"], bfs_result_for_mitm=CFG[\"bfs_result_for_mitm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acd823fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tensor([ 1,  0, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2]): BeamSearchResult(path_found=False),\n",
       " tensor([ 3,  2,  1,  0, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4]): BeamSearchResult(path_found=False),\n",
       " tensor([ 5,  4,  3,  2,  1,  0, 14, 13, 12, 11, 10,  9,  8,  7,  6]): BeamSearchResult(path_found=False),\n",
       " tensor([ 7,  6,  5,  4,  3,  2,  1,  0, 14, 13, 12, 11, 10,  9,  8]): BeamSearchResult(path_found=False),\n",
       " tensor([ 9,  8,  7,  6,  5,  4,  3,  2,  1,  0, 14, 13, 12, 11, 10]): BeamSearchResult(path_found=False),\n",
       " tensor([11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0, 14, 13, 12]): BeamSearchResult(path_found=False),\n",
       " tensor([13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0, 14]): BeamSearchResult(path_length=125, path=X.R.X.L.X.R.R.R.R.R.X.L.X.L.X.R.X.R.X.L.X.R.R.R.X.L.X.R.X.L.L.X.L.X.R.R.X.L.X.L.L.X.R.X.L.L.L.L.X.R.X.R.X.R.X.R.X.R.X.R.X.R.X.R.R.R.X.R.X.R.X.R.X.R.X.L.X.L.X.L.X.L.X.L.X.R.X.R.X.R.X.R.X.R.X.R.X.L.X.L.X.L.X.L.X.L.X.R.X.R.X.R.X.L.X.L.X.R.X.L.L.L.L.L.L),\n",
       " tensor([ 0, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1]): BeamSearchResult(path_found=False),\n",
       " tensor([ 2,  1,  0, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3]): BeamSearchResult(path_found=False),\n",
       " tensor([ 4,  3,  2,  1,  0, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5]): BeamSearchResult(path_found=False),\n",
       " tensor([ 6,  5,  4,  3,  2,  1,  0, 14, 13, 12, 11, 10,  9,  8,  7]): BeamSearchResult(path_found=False),\n",
       " tensor([ 8,  7,  6,  5,  4,  3,  2,  1,  0, 14, 13, 12, 11, 10,  9]): BeamSearchResult(path_found=False),\n",
       " tensor([10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0, 14, 13, 12, 11]): BeamSearchResult(path_length=147, path=L.L.X.L.L.L.L.L.X.L.X.L.L.L.X.L.X.R.X.R.R.X.R.R.X.R.R.X.L.L.L.X.R.X.R.X.L.X.L.X.L.X.L.X.L.L.L.L.X.L.X.R.X.R.R.R.R.R.R.R.R.R.X.L.X.L.X.L.X.L.X.R.X.R.X.R.X.R.X.L.L.L.L.L.L.X.R.X.R.X.R.X.L.L.L.L.X.R.X.R.X.R.X.R.R.X.L.X.R.R.X.R.R.R.X.R.X.R.X.R.X.R.X.L.L.L.X.L.X.R.R.X.R.X.L.L.X.R.X.L.L.L.L.L.L.X.L),\n",
       " tensor([12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0, 14, 13]): BeamSearchResult(path_length=119, path=X.L.X.R.R.R.X.R.R.X.R.R.X.L.X.R.X.R.X.L.X.L.X.L.X.R.X.R.X.R.R.X.L.X.L.X.L.X.L.X.R.R.R.R.R.R.X.R.R.X.R.X.L.X.R.R.X.R.X.L.X.L.X.L.L.X.R.X.R.X.R.X.R.X.R.X.R.X.L.L.X.L.X.L.X.L.X.L.X.L.X.R.X.R.X.R.X.R.X.R.X.R.X.L.X.L.X.L.X.R.X.L.L.X.L.X.L.L.L),\n",
       " tensor([14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0]): BeamSearchResult(path_length=120, path=X.L.X.R.X.R.R.R.X.R.X.L.X.R.R.R.X.L.X.L.X.L.X.L.X.R.X.R.X.R.X.R.X.L.X.L.X.L.X.L.L.L.L.X.R.X.R.X.R.X.R.X.R.X.R.X.R.X.R.R.R.X.R.X.R.X.R.X.R.X.L.X.L.X.L.X.L.X.L.X.R.X.R.X.R.X.R.X.R.X.R.X.L.X.L.X.L.X.L.X.L.X.R.X.R.X.R.X.L.X.L.X.R.X.L.L.L.L.L.L)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_success_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b862a802",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
